W0307 18:50:16.549000 59424 torch/distributed/run.py:793] 
W0307 18:50:16.549000 59424 torch/distributed/run.py:793] *****************************************
W0307 18:50:16.549000 59424 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0307 18:50:16.549000 59424 torch/distributed/run.py:793] *****************************************
2025-03-07 18:50:17,202 - INFO - PyTorch version 2.5.1 available.
2025-03-07 18:50:17,203 - INFO - PyTorch version 2.5.1 available.
2025-03-07 18:50:17,205 - INFO - PyTorch version 2.5.1 available.
2025-03-07 18:50:17,205 - INFO - PyTorch version 2.5.1 available.
2025-03-07 18:50:17,210 - INFO - PyTorch version 2.5.1 available.
2025-03-07 18:50:17,218 - INFO - PyTorch version 2.5.1 available.
2025-03-07 18:50:17,218 - INFO - PyTorch version 2.5.1 available.
2025-03-07 18:50:17,222 - INFO - PyTorch version 2.5.1 available.
[2025-03-07 18:50:19,476] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-07 18:50:19,496] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-07 18:50:19,496] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-07 18:50:19,506] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-07 18:50:19,511] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-07 18:50:19,527] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-07 18:50:19,531] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-07 18:50:19,532] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-07 18:50:21,712] [INFO] [comm.py:652:init_distributed] cdb=None
[W307 18:50:21.989681262 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[2025-03-07 18:50:21,716] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-03-07 18:50:21,716] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W307 18:50:21.992691144 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[2025-03-07 18:50:21,828] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-03-07 18:50:21,833] [INFO] [comm.py:652:init_distributed] cdb=None
[W307 18:50:21.109382923 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-03-07 18:50:21,854 - INFO - Training config: {'model_name': '/gemini/data-1/Qwen2.5-32B-Instruct', 'block_size': 32768, 'wandb_project': 's1', 'wandb_entity': 'hashimoto-group', 'train_file_path': './data/open_s11_32k_1k', 'dagger': False, 'output_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 4.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.05, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct/runs/Mar07_18-50-18_9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 5, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 65, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': '../examples/deepspeed/ds_z3_offload_config.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_text_field': None, 'packing': False, 'max_seq_length': None, 'dataset_num_proc': None, 'dataset_batch_size': 1000, 'model_init_kwargs': None, 'dataset_kwargs': None, 'eval_packing': None, 'num_of_sequences': 1024, 'chars_per_token': 3.6}
[W307 18:50:21.150607629 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[2025-03-07 18:50:21,886] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-03-07 18:50:21,913] [INFO] [comm.py:652:init_distributed] cdb=None
[W307 18:50:21.189728698 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[2025-03-07 18:50:21,916] [INFO] [comm.py:652:init_distributed] cdb=None
[W307 18:50:21.193177245 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[2025-03-07 18:50:21,929] [INFO] [comm.py:652:init_distributed] cdb=None
[W307 18:50:21.205876860 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2025-03-07 18:50:22,039 - INFO - Training config: {'model_name': '/gemini/data-1/Qwen2.5-32B-Instruct', 'block_size': 32768, 'wandb_project': 's1', 'wandb_entity': 'hashimoto-group', 'train_file_path': './data/open_s11_32k_1k', 'dagger': False, 'output_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 4.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.05, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct/runs/Mar07_18-50-18_9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 5, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 65, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 1, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': '../examples/deepspeed/ds_z3_offload_config.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_text_field': None, 'packing': False, 'max_seq_length': None, 'dataset_num_proc': None, 'dataset_batch_size': 1000, 'model_init_kwargs': None, 'dataset_kwargs': None, 'eval_packing': None, 'num_of_sequences': 1024, 'chars_per_token': 3.6}
[2025-03-07 18:50:22,044] [INFO] [comm.py:652:init_distributed] cdb=None
[W307 18:50:22.321450077 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[2025-03-07 18:50:22,069] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
2025-03-07 18:50:22,329 - INFO - Training config: {'model_name': '/gemini/data-1/Qwen2.5-32B-Instruct', 'block_size': 32768, 'wandb_project': 's1', 'wandb_entity': 'hashimoto-group', 'train_file_path': './data/open_s11_32k_1k', 'dagger': False, 'output_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 4.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.05, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct/runs/Mar07_18-50-18_9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 5, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 65, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 2, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': '../examples/deepspeed/ds_z3_offload_config.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_text_field': None, 'packing': False, 'max_seq_length': None, 'dataset_num_proc': None, 'dataset_batch_size': 1000, 'model_init_kwargs': None, 'dataset_kwargs': None, 'eval_packing': None, 'num_of_sequences': 1024, 'chars_per_token': 3.6}
[2025-03-07 18:50:22,362] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
2025-03-07 18:50:22,382 - INFO - Training config: {'model_name': '/gemini/data-1/Qwen2.5-32B-Instruct', 'block_size': 32768, 'wandb_project': 's1', 'wandb_entity': 'hashimoto-group', 'train_file_path': './data/open_s11_32k_1k', 'dagger': False, 'output_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 4.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.05, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct/runs/Mar07_18-50-18_9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 5, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 65, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 4, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': '../examples/deepspeed/ds_z3_offload_config.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_text_field': None, 'packing': False, 'max_seq_length': None, 'dataset_num_proc': None, 'dataset_batch_size': 1000, 'model_init_kwargs': None, 'dataset_kwargs': None, 'eval_packing': None, 'num_of_sequences': 1024, 'chars_per_token': 3.6}
2025-03-07 18:50:22,397 - INFO - Training config: {'model_name': '/gemini/data-1/Qwen2.5-32B-Instruct', 'block_size': 32768, 'wandb_project': 's1', 'wandb_entity': 'hashimoto-group', 'train_file_path': './data/open_s11_32k_1k', 'dagger': False, 'output_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 4.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.05, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct/runs/Mar07_18-50-18_9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 5, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 65, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 7, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': '../examples/deepspeed/ds_z3_offload_config.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_text_field': None, 'packing': False, 'max_seq_length': None, 'dataset_num_proc': None, 'dataset_batch_size': 1000, 'model_init_kwargs': None, 'dataset_kwargs': None, 'eval_packing': None, 'num_of_sequences': 1024, 'chars_per_token': 3.6}
2025-03-07 18:50:22,405 - INFO - Training config: {'model_name': '/gemini/data-1/Qwen2.5-32B-Instruct', 'block_size': 32768, 'wandb_project': 's1', 'wandb_entity': 'hashimoto-group', 'train_file_path': './data/open_s11_32k_1k', 'dagger': False, 'output_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 4.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.05, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct/runs/Mar07_18-50-18_9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 5, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 65, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 3, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': '../examples/deepspeed/ds_z3_offload_config.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_text_field': None, 'packing': False, 'max_seq_length': None, 'dataset_num_proc': None, 'dataset_batch_size': 1000, 'model_init_kwargs': None, 'dataset_kwargs': None, 'eval_packing': None, 'num_of_sequences': 1024, 'chars_per_token': 3.6}
2025-03-07 18:50:22,409 - INFO - Training config: {'model_name': '/gemini/data-1/Qwen2.5-32B-Instruct', 'block_size': 32768, 'wandb_project': 's1', 'wandb_entity': 'hashimoto-group', 'train_file_path': './data/open_s11_32k_1k', 'dagger': False, 'output_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 4.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.05, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct/runs/Mar07_18-50-18_9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 5, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 65, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 5, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': '../examples/deepspeed/ds_z3_offload_config.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_text_field': None, 'packing': False, 'max_seq_length': None, 'dataset_num_proc': None, 'dataset_batch_size': 1000, 'model_init_kwargs': None, 'dataset_kwargs': None, 'eval_packing': None, 'num_of_sequences': 1024, 'chars_per_token': 3.6}
[2025-03-07 18:50:22,414] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
2025-03-07 18:50:22,420 - INFO - Training config: {'model_name': '/gemini/data-1/Qwen2.5-32B-Instruct', 'block_size': 32768, 'wandb_project': 's1', 'wandb_entity': 'hashimoto-group', 'train_file_path': './data/open_s11_32k_1k', 'dagger': False, 'output_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 4.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.05, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct/runs/Mar07_18-50-18_9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 5, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 65, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 6, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': '/model_output/letz/sft/full/open_s11_32k_1k/Qwen2.5-32B-Instruct', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': '../examples/deepspeed/ds_z3_offload_config.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': None, 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_text_field': None, 'packing': False, 'max_seq_length': None, 'dataset_num_proc': None, 'dataset_batch_size': 1000, 'model_init_kwargs': None, 'dataset_kwargs': None, 'eval_packing': None, 'num_of_sequences': 1024, 'chars_per_token': 3.6}
[2025-03-07 18:50:22,452] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[2025-03-07 18:50:22,453] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-03-07 18:50:22,456] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-03-07 18:50:22,458] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-03-07 18:50:32,644] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 771, num_elems = 32.76B
Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:16,  1.06s/it]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:16,  1.03s/it]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:16,  1.03s/it]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:16,  1.04s/it]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:16,  1.04s/it]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:16,  1.04s/it]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:16,  1.05s/it]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:16,  1.05s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:25,  1.70s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:25,  1.70s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:25,  1.70s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:25,  1.70s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:25,  1.71s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:25,  1.71s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:25,  1.70s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:26,  1.79s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:20,  1.48s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:20,  1.48s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:20,  1.48s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:20,  1.48s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:20,  1.49s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:20,  1.48s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:21,  1.51s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:21,  1.50s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.35s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.35s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.35s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.35s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.36s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.36s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.36s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.36s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:06<00:15,  1.30s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:06<00:15,  1.30s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:06<00:15,  1.30s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:06<00:15,  1.31s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:06<00:15,  1.31s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:06<00:15,  1.30s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:06<00:15,  1.31s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:06<00:15,  1.31s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.23s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.23s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.23s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.22s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.23s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.23s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.24s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.24s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:11,  1.17s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:11,  1.17s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:11,  1.17s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:11,  1.17s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:11,  1.17s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:11,  1.18s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:11,  1.18s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:11,  1.18s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:10,  1.20s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:10,  1.20s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:10,  1.20s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:10,  1.20s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:10,  1.20s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:10,  1.20s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:10,  1.21s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:10,  1.21s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:09,  1.22s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:09,  1.22s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:09,  1.22s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:09,  1.22s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:09,  1.22s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:09,  1.22s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:09,  1.23s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:09,  1.23s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:10,  1.56s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:10,  1.56s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:10,  1.56s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:10,  1.56s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:10,  1.56s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:10,  1.57s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:10,  1.57s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:11,  1.58s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:15<00:08,  1.45s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:15<00:08,  1.45s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:15<00:08,  1.45s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:15<00:08,  1.45s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:15<00:08,  1.46s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:15<00:08,  1.46s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:15<00:08,  1.47s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:15<00:08,  1.48s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:17<00:08,  1.79s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:17<00:08,  1.79s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:17<00:08,  1.79s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:17<00:08,  1.79s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:17<00:08,  1.80s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:17<00:09,  1.80s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:17<00:09,  1.80s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:17<00:09,  1.83s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:18<00:06,  1.64s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:18<00:06,  1.64s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:18<00:06,  1.64s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:18<00:06,  1.65s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:18<00:06,  1.65s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:18<00:06,  1.65s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:18<00:06,  1.65s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:18<00:06,  1.63s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:20<00:04,  1.50s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:20<00:04,  1.50s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:20<00:04,  1.50s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:20<00:04,  1.50s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:20<00:04,  1.50s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:20<00:04,  1.50s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:20<00:04,  1.51s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:20<00:04,  1.49s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:21<00:02,  1.40s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:21<00:02,  1.40s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:21<00:02,  1.40s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:21<00:02,  1.40s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:21<00:02,  1.38s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:21<00:02,  1.40s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:21<00:02,  1.40s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:21<00:02,  1.40s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:22<00:01,  1.32s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:22<00:01,  1.32s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:22<00:01,  1.32s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:22<00:01,  1.32s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:22<00:01,  1.32s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:22<00:01,  1.32s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:22<00:01,  1.32s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:22<00:01,  1.31s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]
Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]
Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]
Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]
Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]
Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]
Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]
Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:23<00:00,  1.37s/it]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1000 examples [00:02, 374.85 examples/s]Generating train split: 1000 examples [00:02, 373.92 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:13<00:00, 71.96 examples/s]Map: 100%|██████████| 1000/1000 [00:14<00:00, 69.39 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:13<00:00, 72.98 examples/s]Map: 100%|██████████| 1000/1000 [00:14<00:00, 70.71 examples/s]
2025-03-07 18:51:28,313 - WARNING - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.434274196624756 seconds
Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.472270965576172 seconds
Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4394617080688477 seconds
Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Parameter Offload: Total persistent parameters: 0 in 0 params
Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.434250593185425 seconds
Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.7872753143310547 seconds
Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.8177120685577393 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.8818435668945312 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.8901710510253906 seconds
  0%|          | 0/248 [00:00<?, ?it/s][2025-03-07 18:54:35,111] [WARNING] [stage3.py:2146:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|          | 1/248 [02:15<9:16:28, 135.18s/it][2025-03-07 18:56:38,430] [WARNING] [stage3.py:2146:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 2/248 [04:18<8:45:37, 128.20s/it]  1%|          | 3/248 [06:16<8:23:44, 123.37s/it]  2%|▏         | 4/248 [08:15<8:15:24, 121.82s/it]  2%|▏         | 5/248 [10:17<8:13:45, 121.92s/it]                                                  {'loss': 0.7282, 'grad_norm': 0.5754848122596741, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.08}
  2%|▏         | 5/248 [10:17<8:13:45, 121.92s/it]  2%|▏         | 6/248 [12:19<8:12:10, 122.03s/it]  3%|▎         | 7/248 [14:18<8:05:44, 120.93s/it]  3%|▎         | 8/248 [16:21<8:06:08, 121.54s/it]  4%|▎         | 9/248 [18:21<8:02:53, 121.23s/it]  4%|▍         | 10/248 [20:20<7:57:17, 120.33s/it]                                                   {'loss': 0.6374, 'grad_norm': 0.6838643550872803, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.16}
  4%|▍         | 10/248 [20:20<7:57:17, 120.33s/it]  4%|▍         | 11/248 [22:18<7:53:19, 119.83s/it]  5%|▍         | 12/248 [24:14<7:45:41, 118.40s/it]  5%|▌         | 13/248 [26:13<7:45:18, 118.80s/it]  6%|▌         | 14/248 [28:13<7:44:57, 119.22s/it]  6%|▌         | 15/248 [30:15<7:45:57, 119.99s/it]                                                   {'loss': 0.6224, 'grad_norm': 0.4313904047012329, 'learning_rate': 1.999642588810784e-05, 'epoch': 0.24}
  6%|▌         | 15/248 [30:16<7:45:57, 119.99s/it]  6%|▋         | 16/248 [32:16<7:45:15, 120.33s/it]  7%|▋         | 17/248 [34:17<7:44:04, 120.54s/it]  7%|▋         | 18/248 [36:22<7:46:29, 121.69s/it]  8%|▊         | 19/248 [38:24<7:44:51, 121.80s/it]  8%|▊         | 20/248 [40:27<7:44:01, 122.11s/it]                                                   {'loss': 0.513, 'grad_norm': 0.33321666717529297, 'learning_rate': 1.9956246464468294e-05, 'epoch': 0.32}
  8%|▊         | 20/248 [40:27<7:44:01, 122.11s/it]W0307 19:34:54.811000 59424 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 59493 closing signal SIGTERM
W0307 19:34:54.816000 59424 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 59494 closing signal SIGTERM
W0307 19:34:54.818000 59424 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 59495 closing signal SIGTERM
W0307 19:34:54.819000 59424 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 59496 closing signal SIGTERM
W0307 19:34:54.820000 59424 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 59497 closing signal SIGTERM
W0307 19:34:54.821000 59424 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 59498 closing signal SIGTERM
W0307 19:34:54.823000 59424 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 59499 closing signal SIGTERM
W0307 19:35:24.824000 59424 torch/distributed/elastic/multiprocessing/api.py:916] Unable to shutdown process 59493 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
W0307 19:35:39.425000 59424 torch/distributed/elastic/multiprocessing/api.py:916] Unable to shutdown process 59495 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
E0307 19:35:39.428000 59424 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -9) local_rank: 0 (pid: 59492) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
==========================================================
train/sft.py FAILED
----------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
----------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-07_19:34:54
  host      : 9dde4de879a09719d4ed8dd995ecb9e3-taskrole1-0
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 59492)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 59492
==========================================================
